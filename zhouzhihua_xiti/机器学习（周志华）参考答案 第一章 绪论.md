# 机器学习（周志华）参考答案 第一章 绪论

参考文献 ：http://blog.csdn.net/icefire_tyh/article/details/52065224

1. 表1.1中若只包含编号为1和4的两个样例，试给出相应的版本空间。

表1.1 西瓜数据集

| 编号   | 色泽   | 根蒂   | 敲声   | 好瓜   |
| ---- | ---- | ---- | ---- | ---- |
| 1    | 青绿   | 蜷缩   | 浊响   | 是    |
| 2    | 乌黑   | 蜷缩   | 浊响   | 是    |
| 3    | 青绿   | 硬挺   | 清脆   | 否    |
| 4    | 乌黑   | 稍蜷   | 沉闷   | 否    |

> 假设空间指的是问题所有的假设组成的空间，我们可以把学习过程看做是在假设空间中搜索的过程，搜索目标是寻找与训练集“匹配”的假设。

假设数据集有n种属性，第i个属性可能的取值有$t_{i}$中，加上该属性的泛化取值(*)，所有可能的假设有 $\prod_{i}(t_{i}+1)$ 。再用空集表示没有正例，假设空间中一共有$\prod_{i}(t_{i}+1)+1$ 种假设。

现实问题中常面临很大的假设空间，我们可以寻找一个与训练集一致的假设集合，称之为版本空间。版本空间从假设空间剔除了与正例不一致和 与 反例不一致的假设，它可以看成是对正例的最大泛化。

版本空间的可以通过搜索假设空间来得到，这样需要遍历完整的假设空间。如果数据集中有正例，则可以先对一个正例进行最大泛化，得到$2^{n}$ 个假设，然后再对这些假设进行剔除操作，可以适当精简计算量。

西瓜数据集（精简）

| 编号   | 色泽   | 根蒂   | 敲声   | 好瓜   |
| ---- | ---- | ---- | ---- | ---- |
| 1    | 青绿   | 蜷缩   | 浊响   | 是    |
| 2    | 乌黑   | 稍缩   | 沉闷   | 否    |

数据集有3个属性，每个属性2种取值，一共3 * 3 * 3 + 1 = 28种假设，分别为

- 1.色泽=青绿 根蒂=蜷缩 敲声=浊响
- 2.色泽=青绿 根蒂=蜷缩 敲声=沉闷
- 3.色泽=青绿 根蒂=稍蜷 敲声=浊响
- 4.色泽=青绿 根蒂=稍蜷 敲声=沉闷
- 5.色泽=乌黑 根蒂=蜷缩 敲声=浊响
- 6.色泽=乌黑 根蒂=蜷缩 敲声=沉闷
- 7.色泽=乌黑 根蒂=稍蜷 敲声=浊响
- 8.色泽=乌黑 根蒂=稍蜷 敲声=沉闷
- 9.色泽=青绿 根蒂=蜷缩 敲声=*
- 10.色泽=青绿 根蒂=稍蜷 敲声=*
- 11.色泽=乌黑 根蒂=蜷缩 敲声=*
- 12.色泽=乌黑 根蒂=稍蜷 敲声=*
- 13.色泽=青绿 根蒂=* 敲声=浊响
- 14.色泽=青绿 根蒂=* 敲声=沉闷
- 15.色泽=乌黑 根蒂=* 敲声=浊响
- 16.色泽=乌黑 根蒂=* 敲声=沉闷
- 17.色泽=* 根蒂=蜷缩 敲声=浊响
- 18.色泽=* 根蒂=蜷缩 敲声=沉闷
- 19.色泽=* 根蒂=稍蜷 敲声=浊响
- 20.色泽=* 根蒂=稍蜷 敲声=沉闷
- 21.色泽=青绿 根蒂=* 敲声=*
- 22.色泽=乌黑 根蒂=* 敲声=*
- 23.色泽=* 根蒂=蜷缩 敲声=*
- 24.色泽=* 根蒂=稍蜷 敲声=*
- 25.色泽=* 根蒂=* 敲声=浊响
- 26.色泽=* 根蒂=* 敲声=沉闷
- 27.色泽=* 根蒂=* 敲声=*
- 28.空集 Ø 

编号为1的数据删除属性中色泽为乌黑，根蒂为稍蜷，敲声为沉闷的数据

所以版本空间为

- 1.色泽=青绿 根蒂=蜷缩 敲声=浊响
- 9.色泽=青绿 根蒂=蜷缩 敲声=*
- 13.色泽=青绿 根蒂=* 敲声=浊响
- 17.色泽=* 根蒂=蜷缩 敲声=浊响
- 21.色泽=青绿 根蒂=* 敲声=*
- 23.色泽=* 根蒂=蜷缩 敲声=*
- 25.色泽=* 根蒂=* 敲声=浊响 

一般情况下版本空间是正例的泛化，但由于数据集中只有一个正例，所以在版本空间中依然包含了这个样本的假设。

2. 与使用单个合取式来进行假设表示相比，使用“析合范式”将使得假设空间具有更强的表示能力。若使用最多包含k个合取式的析合范式来表达1.1的西瓜分类问题的假设空间，试估算有多少种可能的假设。

表1.1 包含4个样例，3种属性，假设空间中有3 * 4 * 4 + 1 = 49种假设。在不考虑冗余的情况下，最多包含k个合取式来表达假设空间，显然k的最大值是49，每次从中选出k个来组成析合式，共$ΣC_{49}^k=2^{49}$种可能。但是其中包含了很多沉余的情况(至少存在一个合取式被剩余的析合式完全包含<空集除外>)。

如果考虑沉余的情况 
在这里忽略空集，一个原因是并不是太明白空集是否应该加入析合式，另外就算需要加入，求出了前面48种假设的组合，可以很容易求出加入空集后的组合数(每种可能都可以加上空集，再加上1种空集单独的情况)。 
48种假设中： 
具体假设：2∗3∗3=18种 
一个属性泛化假设：2∗3+3∗3+2∗3=21种 
两个属性泛化假设：2+3+3=8种 
三属性泛化：1种 

3. 若数据包含噪声，则假设空间中可能不存在与所有的训练样本都一致的假设。在此情形下，试设计一种归纳偏好用于假设选择。

通常认为两个数据的属性越相近，则更倾向于将他们分为同一类。若相同属性出现了两种不同的分类，则认为它属于与他最临近几个数据的属性。也可以考虑同时去掉所有具有相同属性而不同分类的数据，留下的数据就是没有误差的数据，但是可能会丢失部分信息。

4. 本章1.4节在论述“没有免费的午餐”定理时，默认使用了“分类错误率”作为性能度量来对分类器进行评估。若换用其他性能度量$l$ ，试证明没有免费的午餐定理仍成立。

还是考虑二分类的问题， NFL首先要保证真实目标函数$f$ 均匀分布，对于有$X$个样本的二分类问题，显然$f$共有$2^X$ 种情况。其中一半是与假设一致的，也就$P(f(x)=h(x))=0.5$

此时，$∑_fl(h(x),f(x))=0.5* 2^X*(l(h(x)=f(x))+l(h(x)≠f(x)))$

$l(h(x)=f(x))+l(h(x)≠f(x))$ 应该是个常数，隐含的条件就是（一个比较合理的充分条件）$l(0,0)=l(1,1),l(1,0)=l(0,1)$ 。如果不满足，NFL不成立。

5. 试述机器学习能在互联网搜索的哪些环节起什么作用。

* 最常见的，消息推送，比如淘宝发送的某些感兴趣的商品。（商品推荐，物品推荐）
* 网站相关度排行，通过点击量，网页内容进行综合分析。
* 图片搜索，现在大部分是通过标签来搜索，不过基于像素的搜索总会有把。